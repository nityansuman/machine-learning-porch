{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation System: Ranking Stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real-world recommender systems are often composed of two stages:\n",
    "\n",
    "- The retrieval stage is responsible for selecting an initial set of hundreds of candidates from all possible candidates. The main objective of this model is to efficiently weed out all candidates that the user is not interested in. Because the retrieval model may be dealing with millions of candidates, it has to be computationally efficient.\n",
    "- The ranking stage takes the outputs of the retrieval model and fine-tunes them to select the best possible handful of recommendations. Its task is to narrow down the set of items the user may be interested in to a shortlist of likely candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pprint import pprint\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['movielens', 'datasets', 'tiny_shakespeare', 'imdb_reviews', 'downloads']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"/database/tensorflow-datasets/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ranking stage, ratings will be used as the objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-10 21:18:15.504202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-10 21:18:15.509574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-10 21:18:15.509937: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-10 21:18:15.510555: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-10 21:18:15.511736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-10 21:18:15.512201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-10 21:18:15.512616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-10 21:18:15.924365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-10 21:18:15.924636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-10 21:18:15.924857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-10 21:18:15.925069: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 314 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:2d:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "ratings = tfds.load(\"movielens/100k-ratings\", split=\"train\", data_dir=\"/database/tensorflow-datasets/\")\n",
    "\n",
    "ratings = ratings.map(lambda x: {\n",
    "    \"movie_title\": x[\"movie_title\"],\n",
    "    \"user_id\": x[\"user_id\"],\n",
    "    \"user_rating\": x[\"user_rating\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movie_title': b\"One Flew Over the Cuckoo's Nest (1975)\",\n",
      " 'user_id': b'138',\n",
      " 'user_rating': 4.0}\n",
      "{'movie_title': b'Strictly Ballroom (1992)',\n",
      " 'user_id': b'92',\n",
      " 'user_rating': 2.0}\n",
      "{'movie_title': b'Very Brady Sequel, A (1996)',\n",
      " 'user_id': b'301',\n",
      " 'user_rating': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-10 21:18:16.132953: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for x in ratings.take(3).as_numpy_iterator():\n",
    "\tpprint(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test split\n",
    "tf.random.set_seed(42)\n",
    "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(80_000)\n",
    "test = shuffled.skip(80_000).take(20_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_titles = ratings.batch(1_000_000).map(lambda x: x[\"movie_title\"])\n",
    "user_ids = ratings.batch(1_000_000).map(lambda x: x[\"user_id\"])\n",
    "\n",
    "unique_movie_titles = np.unique(np.concatenate(list(movie_titles)))\n",
    "unique_user_ids = np.unique(np.concatenate(list(user_ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankingModel(tf.keras.Model):\n",
    "\tdef __init__(self, embed_dim = 64) -> None:\n",
    "\t\tsuper().__init__()\n",
    "\t\tembedding_dimension = embed_dim\n",
    "\t\tself.user_embeddings = tf.keras.Sequential([\n",
    "\t\t\ttf.keras.layers.StringLookup(vocabulary=unique_user_ids, mask_token=None),\n",
    "\t\t\ttf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)\n",
    "\t\t])\n",
    "\t\tself.movie_embeddings = tf.keras.Sequential([\n",
    "\t\t\ttf.keras.layers.StringLookup(vocabulary=unique_movie_titles, mask_token=None),\n",
    "\t\t\ttf.keras.layers.Embedding(len(unique_movie_titles) + 1, embedding_dimension)\n",
    "\t\t])\n",
    "\t\tself.ratings = tf.keras.Sequential([\n",
    "\t\t\ttf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "\t\t\ttf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "\t\t\ttf.keras.layers.Dense(1)\n",
    "\t\t])\n",
    "\n",
    "\tdef call(self, inputs):\n",
    "\t\tuser_id, movie_title = inputs\n",
    "\t\tuser_embedding = self.user_embeddings(user_id)\n",
    "\t\tmovie_embedding = self.movie_embeddings(movie_title)\n",
    "\t\treturn self.ratings(tf.concat([user_embedding, movie_embedding], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model takes user_id and movie titles and output a predicted rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'list'> input: ['42']\n",
      "Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'list'> input: ['42']\n",
      "Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'list'> input: [\"One Flew Over the Cuckoo's Nest (1975)\"]\n",
      "Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'list'> input: [\"One Flew Over the Cuckoo's Nest (1975)\"]\n",
      "Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.00769137]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RankingModel()(([\"42\"], [\"One Flew Over the Cuckoo's Nest (1975)\"])) # Without training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make use of the Ranking task object: a convenience wrapper that bundles together the loss function and metric computation and also use it together with the MeanSquaredError Keras loss in order to predict the ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = tfrs.tasks.Ranking(\n",
    "\tloss = tf.keras.losses.MeanSquaredError(),\n",
    "\tmetrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task itself is a Keras layer that takes true and predicted as arguments, and returns the computed loss.\n",
    "\n",
    "Now we put all this together into a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovielensModel(tfrs.models.Model):\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.ranking_model = RankingModel()\n",
    "\t\tself.task: tf.keras.layers.Layer = tfrs.tasks.Ranking(\n",
    "\t\t\tloss = tf.keras.losses.MeanSquaredError(),\n",
    "\t\t\tmetrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
    "\t\t)\n",
    "\n",
    "\tdef call(self, features):\n",
    "\t\treturn self.ranking_model((features[\"user_id\"], features[\"movie_title\"]))\n",
    "\n",
    "\tdef compute_loss(self, features, training=False):\n",
    "\t\tlabels = features.pop(\"user_rating\")\n",
    "\t\trating_predictions = self(features)\n",
    "\t\treturn self.task(labels=labels, predictions=rating_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn and eveluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MovielensModel()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_train = train.shuffle(100_000).batch(8192).cache()\n",
    "cached_test = test.batch(4096).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10/10 [==============================] - 2s 86ms/step - root_mean_squared_error: 2.1331 - loss: 4.1831 - regularization_loss: 0.0000e+00 - total_loss: 4.1831 - val_root_mean_squared_error: 1.1259 - val_loss: 1.2373 - val_regularization_loss: 0.0000e+00 - val_total_loss: 1.2373\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 0s 7ms/step - root_mean_squared_error: 1.1190 - loss: 1.2511 - regularization_loss: 0.0000e+00 - total_loss: 1.2511 - val_root_mean_squared_error: 1.1108 - val_loss: 1.2068 - val_regularization_loss: 0.0000e+00 - val_total_loss: 1.2068\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 0s 7ms/step - root_mean_squared_error: 1.1065 - loss: 1.2237 - regularization_loss: 0.0000e+00 - total_loss: 1.2237 - val_root_mean_squared_error: 1.0997 - val_loss: 1.1854 - val_regularization_loss: 0.0000e+00 - val_total_loss: 1.1854\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 0s 7ms/step - root_mean_squared_error: 1.0933 - loss: 1.1942 - regularization_loss: 0.0000e+00 - total_loss: 1.1942 - val_root_mean_squared_error: 1.0848 - val_loss: 1.1565 - val_regularization_loss: 0.0000e+00 - val_total_loss: 1.1565\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 0s 7ms/step - root_mean_squared_error: 1.0752 - loss: 1.1545 - regularization_loss: 0.0000e+00 - total_loss: 1.1545 - val_root_mean_squared_error: 1.0660 - val_loss: 1.1202 - val_regularization_loss: 0.0000e+00 - val_total_loss: 1.1202\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 0s 7ms/step - root_mean_squared_error: 1.0549 - loss: 1.1115 - regularization_loss: 0.0000e+00 - total_loss: 1.1115 - val_root_mean_squared_error: 1.0490 - val_loss: 1.0880 - val_regularization_loss: 0.0000e+00 - val_total_loss: 1.0880\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 0s 8ms/step - root_mean_squared_error: 1.0372 - loss: 1.0744 - regularization_loss: 0.0000e+00 - total_loss: 1.0744 - val_root_mean_squared_error: 1.0339 - val_loss: 1.0594 - val_regularization_loss: 0.0000e+00 - val_total_loss: 1.0594\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 0s 7ms/step - root_mean_squared_error: 1.0185 - loss: 1.0357 - regularization_loss: 0.0000e+00 - total_loss: 1.0357 - val_root_mean_squared_error: 1.0145 - val_loss: 1.0214 - val_regularization_loss: 0.0000e+00 - val_total_loss: 1.0214\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 0s 7ms/step - root_mean_squared_error: 0.9984 - loss: 0.9953 - regularization_loss: 0.0000e+00 - total_loss: 0.9953 - val_root_mean_squared_error: 0.9969 - val_loss: 0.9870 - val_regularization_loss: 0.0000e+00 - val_total_loss: 0.9870\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 0s 7ms/step - root_mean_squared_error: 0.9824 - loss: 0.9640 - regularization_loss: 0.0000e+00 - total_loss: 0.9640 - val_root_mean_squared_error: 0.9844 - val_loss: 0.9632 - val_regularization_loss: 0.0000e+00 - val_total_loss: 0.9632\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f181c0df490>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(cached_train, epochs=10, validation_data=cached_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step - root_mean_squared_error: 0.9844 - loss: 0.9680 - regularization_loss: 0.0000e+00 - total_loss: 0.9680\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'root_mean_squared_error': 0.9844292402267456,\n",
       " 'loss': 0.9631555080413818,\n",
       " 'regularization_loss': 0,\n",
       " 'total_loss': 0.9631555080413818}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(cached_test, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the ranking model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratings:\n",
      "M*A*S*H (1970): [[3.8500729]]\n",
      "Dances with Wolves (1990): [[3.6245043]]\n",
      "Speed (1994): [[3.5271575]]\n"
     ]
    }
   ],
   "source": [
    "test_ratings = {}\n",
    "test_movie_titles = [\"M*A*S*H (1970)\", \"Dances with Wolves (1990)\", \"Speed (1994)\"]\n",
    "\n",
    "for movie_title in test_movie_titles:\n",
    "\ttest_ratings[movie_title] = model({\n",
    "\t\t\"user_id\": np.array([\"42\"]),\n",
    "\t\t\"movie_title\": np.array([movie_title])\n",
    "\t})\n",
    "\n",
    "print(\"Ratings:\")\n",
    "for title, score in sorted(test_ratings.items(), key=lambda x: x[1], reverse=True):\n",
    "\tprint(f\"{title}: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model above gives us a decent start towards building a ranking system and a careful understanding of the objectives worth optimizing is also necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

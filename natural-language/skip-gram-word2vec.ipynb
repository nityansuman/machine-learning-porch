{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T00:40:40.793094Z",
     "start_time": "2021-02-08T00:40:39.875264Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import packages\n",
    "import io\n",
    "import string\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "tf.__version__ # 2.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-Gram Model\n",
    "\n",
    "Word2Vec is not a singular algorithm, rather, it is a family of model architectures and optimizations that can be used to learn word embeddings from large datasets.\n",
    "\n",
    "These papers proposed two methods for learning representations of words:\n",
    "\n",
    "- Continuous Bag-of-Words Model which predicts the middle word based on surrounding context words. The context consists of a few words before and after the current (middle) word. This architecture is called a bag-of-words model as the order of words in the context is not important.\n",
    "\n",
    "- Continuous Skip-gram Model which predict words within a certain range before and after the current word in the same sentence.\n",
    "\n",
    "A skip-gram model predicts the context (or neighbors) of a word, given the word itself.\n",
    "The model is trained on skip-grams, which are n-grams that allows tokens to be skipped.\n",
    "The context of a word can be representend using a pair of (`target_words`, `context_word`) where `context_word` appears in the neighboring context of the `target_word`.\n",
    "\n",
    "The `window_size` here determines the number of neighbors on either side of the `target_word` for context.\n",
    "\n",
    "The objective of the skip-gram model is to maximize the probability of predicting context words given the `target_word`.\n",
    "\n",
    "The computation of probability here would require to take softmax over the entire vocabulary which is often very large as any word from the vocabulary can be predicted for the given `target_word`. The `Noise Contrastive Estimation` loss function is an efficient approximation for a full softmax. Rather than taking the whole vocabulary as sample for probability distribution, negative sampling is used.\n",
    "\n",
    "The simplified negative sampling objective for a `target_word` is to distinguish the context words from number of n negative samples drawn from a noise distribution P(W) of words. That is, an approximation of softmax over the entire vocabulary is to pose the loss for a `target_word` as a classification problem between context word and number of n negative samples.\n",
    "\n",
    "A negative sample is defined as (`target_word`, `context_word`) pair such that the context word does not appear in the `window_size` of the `target_word`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T00:40:40.802207Z",
     "start_time": "2021-02-08T00:40:40.794429Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n"
     ]
    }
   ],
   "source": [
    "PATH_TO_FILE = tf.keras.utils.get_file(\"shakespeare\", \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\")\n",
    "\n",
    "# Load dataset\n",
    "with open(PATH_TO_FILE) as f: \n",
    "    lines = f.read().splitlines()\n",
    "    for line in lines[:20]:\n",
    "        print(line)\n",
    "\n",
    "# Create dataset\n",
    "text_ds = tf.data.TextLineDataset(PATH_TO_FILE).filter(lambda x: tf.cast(tf.strings.length(x), bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T00:40:41.316658Z",
     "start_time": "2021-02-08T00:40:40.803694Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: tf.Tensor(b'First Citizen:', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for text in text_ds.take(1):\n",
    "    print(\"Input text:\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T00:40:42.136306Z",
     "start_time": "2021-02-08T00:40:41.328112Z"
    }
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 5000\n",
    "MAX_SEQUENCE_LENGTH = 32\n",
    "\n",
    "# Define a vectorization layer\n",
    "encoder = TextVectorization(\n",
    "    standardize=\"lower_and_strip_punctuation\",\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "    output_mode=\"int\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T00:40:42.179061Z",
     "start_time": "2021-02-08T00:40:42.141923Z"
    }
   },
   "outputs": [],
   "source": [
    "# Adapt using the training dataset\n",
    "encoder.adapt(text_ds.batch(132))\n",
    "\n",
    "# Vectorize the data using the learned encoder\n",
    "text_vector_ds = text_ds.batch(1024).prefetch(tf.data.experimental.AUTOTUNE).map(encoder).unbatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T00:40:43.896222Z",
     "start_time": "2021-02-08T00:40:42.180230Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 32777\n",
      "First sequence: [ 89 270   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# Get sequences out\n",
    "sequences = list(text_vector_ds.as_numpy_iterator())\n",
    "\n",
    "print(\"Number of sequences:\", len(sequences))\n",
    "print(\"First sequence:\", sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T00:40:51.346370Z",
     "start_time": "2021-02-08T00:40:43.900715Z"
    }
   },
   "outputs": [],
   "source": [
    "SEED = 2021\n",
    "NUM_NS = 4\n",
    "WINDOW_SIZE = 2\n",
    "targets, contexts, labels = list(), list(), list()\n",
    "\n",
    "# Create subsampling table\n",
    "subsampling_table = tf.keras.preprocessing.sequence.make_sampling_table(size=VOCAB_SIZE)\n",
    "\n",
    "# Create targets, contexts and labels from the sequence\n",
    "for sequence in sequences:\n",
    "    # Get positive ngrams\n",
    "    positive_ngrams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "        sequence=sequence,\n",
    "        vocabulary_size=VOCAB_SIZE,\n",
    "        sampling_table=subsampling_table,\n",
    "        window_size=WINDOW_SIZE,\n",
    "        negative_samples=0\n",
    "    )\n",
    "\n",
    "    for target_w, context_w in positive_ngrams:\n",
    "        # Create context tensor\n",
    "        context_class = tf.expand_dims(tf.constant([context_w], dtype=\"int64\"), 1)\n",
    "\n",
    "        # Get negative contexts\n",
    "        negative_context_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "            true_classes=context_class,\n",
    "            num_true=1,\n",
    "            num_sampled=NUM_NS,\n",
    "            unique=True,\n",
    "            range_max=VOCAB_SIZE,\n",
    "            seed=SEED\n",
    "        )\n",
    "        negative_context_candidates = tf.expand_dims(negative_context_candidates, 1)\n",
    "\n",
    "        # Add negative and positive contexts together\n",
    "        context = tf.concat([context_class, negative_context_candidates], 0)\n",
    "\n",
    "        # Create label\n",
    "        label = tf.constant([1] + [0]*NUM_NS, dtype=\"int64\")\n",
    "\n",
    "        # Store\n",
    "        targets.append(target_w)\n",
    "        contexts.append(context)\n",
    "        labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T00:40:53.045124Z",
     "start_time": "2021-02-08T00:40:51.350974Z"
    }
   },
   "outputs": [],
   "source": [
    "BUFFER = 10000\n",
    "BUFFER_BS = 64\n",
    "\n",
    "# Create optimized dataset for model\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "dataset = dataset.shuffle(BUFFER).batch(BUFFER_BS, drop_remainder=True)\n",
    "dataset = dataset.cache().prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "The Word2Vec model can be implemented as a classifier to distinguish between true context words from skip-grams and false context words obtained through negative sampling.\n",
    "\n",
    "You can perform a dot product between the embeddings of target and context words to obtain predictions for labels and compute loss against true labels in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T00:40:53.051209Z",
     "start_time": "2021-02-08T00:40:53.046355Z"
    }
   },
   "outputs": [],
   "source": [
    "class Word2Vec(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_ns):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.target_embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=1, name=\"w2v_embedding\")\n",
    "        self.context_embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=num_ns+1)\n",
    "        self.dots = tf.keras.layers.Dot(axes=(3,2))\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "    \n",
    "    def call(self, pair):\n",
    "        target, context = pair\n",
    "        we = self.target_embedding(target)\n",
    "        ce = self.context_embedding(context)\n",
    "        dots = self.dots([ce, we])\n",
    "        return self.flatten(dots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T00:40:53.079339Z",
     "start_time": "2021-02-08T00:40:53.051937Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "   1/1105 [..............................] - ETA: 0s - loss: 1.6108 - accuracy: 0.1406WARNING:tensorflow:From /home/nityan/anaconda3/envs/dl-env/lib/python3.8/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "1105/1105 [==============================] - 5s 5ms/step - loss: 1.5708 - accuracy: 0.2890\n",
      "Epoch 2/5\n",
      "1105/1105 [==============================] - 5s 5ms/step - loss: 1.3814 - accuracy: 0.4588\n",
      "Epoch 3/5\n",
      "1105/1105 [==============================] - 5s 5ms/step - loss: 1.1592 - accuracy: 0.5941\n",
      "Epoch 4/5\n",
      "1105/1105 [==============================] - 5s 5ms/step - loss: 0.9473 - accuracy: 0.7051\n",
      "Epoch 5/5\n",
      "1105/1105 [==============================] - 5s 5ms/step - loss: 0.7651 - accuracy: 0.7826\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "\n",
    "# Instantiate model\n",
    "word2vec = Word2Vec(VOCAB_SIZE, EMBEDDING_DIM, NUM_NS)\n",
    "\n",
    "# Compile model\n",
    "word2vec.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Log modelling with tensorboard for analysis and visualization\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
    "\n",
    "# Fit model on train dataset\n",
    "history = word2vec.fit(dataset, epochs=5, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T00:41:36.394535Z",
     "start_time": "2021-02-08T00:41:04.986861Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# View embeddings on Tensorboard\n",
    "# ! tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T00:40:59.318070Z",
     "start_time": "2021-02-08T00:40:59.240127Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get weights from layer\n",
    "weights = word2vec.get_layer(\"w2v_embedding\").get_weights()[0]\n",
    "vocab = encoder.get_vocabulary()\n",
    "\n",
    "# Create and save metadata files\n",
    "out_v = io.open(\"/tmp/vectors.tsv\", \"w\", encoding=\"utf-8\")\n",
    "out_m = io.open(\"/tmp/metadata.tsv\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "    if  index == 0: continue # skip 0, it\"s padding.\n",
    "    vec = weights[index] \n",
    "    out_v.write(\"\\t\".join([str(x) for x in vec]) + \"\\n\")\n",
    "    out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dl-env]",
   "language": "python",
   "name": "conda-env-dl-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "147px",
    "width": "308px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

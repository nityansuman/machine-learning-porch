{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation System: Retrieval Stage\n",
    "\n",
    "Retrieval models are often composed of two sub-models:\n",
    "\n",
    "- A query model computing the query representation (normally a fixed-dimensionality embedding vector) using query features.\n",
    "- A candidate model computing the candidate representation (an equally-sized vector) using the candidate features\n",
    "\n",
    "The outputs of the two models are then multiplied together to give a query-candidate affinity score, with higher scores expressing a better match between the candidate and the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pprint import pprint\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "from typing import Dict, Text\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['movielens', 'datasets', 'tiny_shakespeare', 'imdb_reviews', 'downloads']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"/database/tensorflow-datasets/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-10 19:53:35.479201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-10 19:53:35.483276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-10 19:53:35.483725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-10 19:53:35.484593: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-10 19:53:35.485394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-10 19:53:35.486306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-10 19:53:35.487414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-10 19:53:35.913263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-10 19:53:35.913732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-10 19:53:35.914136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-10 19:53:35.914530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9183 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:2d:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "ratings = tfds.load(\"movielens/100k-ratings\", split=\"train\", data_dir=\"/database/tensorflow-datasets/\")\n",
    "movies = tfds.load(\"movielens/100k-movies\", split=\"train\", data_dir=\"/database/tensorflow-datasets/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bucketized_user_age': 45.0,\n",
      " 'movie_genres': array([7]),\n",
      " 'movie_id': b'357',\n",
      " 'movie_title': b\"One Flew Over the Cuckoo's Nest (1975)\",\n",
      " 'raw_user_age': 46.0,\n",
      " 'timestamp': 879024327,\n",
      " 'user_gender': True,\n",
      " 'user_id': b'138',\n",
      " 'user_occupation_label': 4,\n",
      " 'user_occupation_text': b'doctor',\n",
      " 'user_rating': 4.0,\n",
      " 'user_zip_code': b'53211'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-10 19:53:36.202170: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for x in ratings.take(1).as_numpy_iterator():\n",
    "\tpprint(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movie_genres': array([4]),\n",
      " 'movie_id': b'1681',\n",
      " 'movie_title': b'You So Crazy (1994)'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-10 19:53:36.294936: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for x in movies.take(1).as_numpy_iterator():\n",
    "\tpprint(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this iteration, keeping only `movie_title` and `user_id` information\n",
    "ratings = ratings.map(lambda x: {\n",
    "\t\"movie_title\": x[\"movie_title\"],\n",
    "\t\"user_id\": x[\"user_id\"],\n",
    "})\n",
    "movies = movies.map(lambda x: x[\"movie_title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test set (ideally based on time) using random split\n",
    "tf.random.set_seed(42)\n",
    "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(80_000)\n",
    "test = shuffled.skip(80_000).take(20_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movie_title': b'Postman, The (1997)', 'user_id': b'681'}\n"
     ]
    }
   ],
   "source": [
    "for x in train.take(1).as_numpy_iterator():\n",
    "\tpprint(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movie_title': b'M*A*S*H (1970)', 'user_id': b'346'}\n"
     ]
    }
   ],
   "source": [
    "for x in test.take(1).as_numpy_iterator():\n",
    "\tpprint(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b\"'Til There Was You (1997)\" b'1-900 (1994)' b'101 Dalmatians (1996)'\n",
      " b'12 Angry Men (1957)' b'187 (1997)' b'2 Days in the Valley (1996)'\n",
      " b'20,000 Leagues Under the Sea (1954)' b'2001: A Space Odyssey (1968)'\n",
      " b'3 Ninjas: High Noon At Mega Mountain (1998)' b'39 Steps, The (1935)']\n",
      "[b'1' b'10' b'100' b'101' b'102']\n"
     ]
    }
   ],
   "source": [
    "# Get unique movies and user_id present in the data\n",
    "movie_titles = movies.batch(1000)\n",
    "user_ids = ratings.batch(1_000_000).map(lambda x: x[\"user_id\"])\n",
    "\n",
    "unique_movie_titles = np.unique(np.concatenate(list(movie_titles)))\n",
    "unique_user_ids = np.unique(np.concatenate(list(user_ids)))\n",
    "\n",
    "print(unique_movie_titles[:10])\n",
    "print(unique_user_ids[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement model\n",
    "\n",
    "Choosing the architecture of our model is a key part of modelling.\n",
    "\n",
    "Because we are building a two-tower retrieval model, we can build each tower separately and then combine them in the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set embedding dimension\n",
    "embedding_dimension = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define user model\n",
    "user_model = tf.keras.Sequential([\n",
    "\ttf.keras.layers.StringLookup(vocabulary=unique_user_ids, mask_token=None),\n",
    "\ttf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension),\n",
    "\ttf.keras.layers.BatchNormalization()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define candiate model\n",
    "movie_model = tf.keras.Sequential([\n",
    "\ttf.keras.layers.StringLookup(vocabulary=unique_movie_titles, mask_token=None),\n",
    "\ttf.keras.layers.Embedding(len(unique_movie_titles) + 1, embedding_dimension),\n",
    "\ttf.keras.layers.BatchNormalization()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our training data we have positive (user, movie) pairs. To figure out how good our model is, we need to compare the affinity score that the model calculates for this pair to the scores of all the other possible candidates: if the score for the positive pair is higher than for all other candidates, our model is highly accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set metric\n",
    "metrics = tfrs.metrics.FactorizedTopK(candidates=movies.batch(256).map(movie_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set objective\n",
    "retrieval_task = tfrs.tasks.Retrieval(metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task itself is a Keras layer that takes the query and candidate embeddings as arguments, and returns the computed loss: we'll use that to implement the model's training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the candidate and user model to build the complete retrieval model\n",
    "class MovielensModel(tfrs.Model):\n",
    "\tdef __init__(self, user_model, movie_model, retrieval_task):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.movie_model: tf.keras.Model = movie_model\n",
    "\t\tself.user_model: tf.keras.Model = user_model\n",
    "\t\tself.task: tf.keras.layers.Layer = retrieval_task\n",
    "\t\n",
    "\tdef compute_loss(self, features, training=False):\n",
    "\t\tuser_embeddings = self.user_model(features[\"user_id\"])\n",
    "\t\tpositive_movie_embeddings = self.movie_model(features[\"movie_title\"])\n",
    "\t\treturn self.task(user_embeddings, positive_movie_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tfrs.Model base class is a simply convenience class: it allows us to compute both training and test losses using the same method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get combined model\n",
    "main_model = MovielensModel(user_model, movie_model, retrieval_task)\n",
    "\n",
    "# Compile model\n",
    "main_model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now shuffle, batch and cache training and evaluation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_train = train.shuffle(100_000).batch(4096).cache()\n",
    "cached_test = test.batch(4096).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "20/20 [==============================] - 6s 221ms/step - factorized_top_k/top_1_categorical_accuracy: 6.1250e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0048 - factorized_top_k/top_10_categorical_accuracy: 0.0113 - factorized_top_k/top_50_categorical_accuracy: 0.0698 - factorized_top_k/top_100_categorical_accuracy: 0.1440 - loss: 31940.0115 - regularization_loss: 0.0000e+00 - total_loss: 31940.0115 - val_factorized_top_k/top_1_categorical_accuracy: 0.0016 - val_factorized_top_k/top_5_categorical_accuracy: 0.0098 - val_factorized_top_k/top_10_categorical_accuracy: 0.0229 - val_factorized_top_k/top_50_categorical_accuracy: 0.1254 - val_factorized_top_k/top_100_categorical_accuracy: 0.2418 - val_loss: 28524.4023 - val_regularization_loss: 0.0000e+00 - val_total_loss: 28524.4023\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 3s 175ms/step - factorized_top_k/top_1_categorical_accuracy: 9.6250e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0126 - factorized_top_k/top_10_categorical_accuracy: 0.0285 - factorized_top_k/top_50_categorical_accuracy: 0.1482 - factorized_top_k/top_100_categorical_accuracy: 0.2706 - loss: 30618.5096 - regularization_loss: 0.0000e+00 - total_loss: 30618.5096 - val_factorized_top_k/top_1_categorical_accuracy: 7.5000e-04 - val_factorized_top_k/top_5_categorical_accuracy: 0.0074 - val_factorized_top_k/top_10_categorical_accuracy: 0.0171 - val_factorized_top_k/top_50_categorical_accuracy: 0.1104 - val_factorized_top_k/top_100_categorical_accuracy: 0.2159 - val_loss: 28318.1602 - val_regularization_loss: 0.0000e+00 - val_total_loss: 28318.1602\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 3s 174ms/step - factorized_top_k/top_1_categorical_accuracy: 7.7500e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0170 - factorized_top_k/top_10_categorical_accuracy: 0.0384 - factorized_top_k/top_50_categorical_accuracy: 0.1817 - factorized_top_k/top_100_categorical_accuracy: 0.3116 - loss: 30000.9932 - regularization_loss: 0.0000e+00 - total_loss: 30000.9932 - val_factorized_top_k/top_1_categorical_accuracy: 7.0000e-04 - val_factorized_top_k/top_5_categorical_accuracy: 0.0044 - val_factorized_top_k/top_10_categorical_accuracy: 0.0122 - val_factorized_top_k/top_50_categorical_accuracy: 0.0943 - val_factorized_top_k/top_100_categorical_accuracy: 0.1969 - val_loss: 28490.2461 - val_regularization_loss: 0.0000e+00 - val_total_loss: 28490.2461\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 3s 175ms/step - factorized_top_k/top_1_categorical_accuracy: 6.3750e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0205 - factorized_top_k/top_10_categorical_accuracy: 0.0452 - factorized_top_k/top_50_categorical_accuracy: 0.2028 - factorized_top_k/top_100_categorical_accuracy: 0.3407 - loss: 29619.7119 - regularization_loss: 0.0000e+00 - total_loss: 29619.7119 - val_factorized_top_k/top_1_categorical_accuracy: 4.0000e-04 - val_factorized_top_k/top_5_categorical_accuracy: 0.0034 - val_factorized_top_k/top_10_categorical_accuracy: 0.0090 - val_factorized_top_k/top_50_categorical_accuracy: 0.0838 - val_factorized_top_k/top_100_categorical_accuracy: 0.1854 - val_loss: 28707.3379 - val_regularization_loss: 0.0000e+00 - val_total_loss: 28707.3379\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 3s 176ms/step - factorized_top_k/top_1_categorical_accuracy: 4.3750e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0231 - factorized_top_k/top_10_categorical_accuracy: 0.0507 - factorized_top_k/top_50_categorical_accuracy: 0.2183 - factorized_top_k/top_100_categorical_accuracy: 0.3580 - loss: 29363.7118 - regularization_loss: 0.0000e+00 - total_loss: 29363.7118 - val_factorized_top_k/top_1_categorical_accuracy: 2.5000e-04 - val_factorized_top_k/top_5_categorical_accuracy: 0.0022 - val_factorized_top_k/top_10_categorical_accuracy: 0.0070 - val_factorized_top_k/top_50_categorical_accuracy: 0.0789 - val_factorized_top_k/top_100_categorical_accuracy: 0.1774 - val_loss: 28888.2402 - val_regularization_loss: 0.0000e+00 - val_total_loss: 28888.2402\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcc4041f070>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "main_model.fit(\n",
    "\tcached_train, epochs=5, validation_data=cached_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 133ms/step - factorized_top_k/top_1_categorical_accuracy: 2.5000e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0022 - factorized_top_k/top_10_categorical_accuracy: 0.0070 - factorized_top_k/top_50_categorical_accuracy: 0.0789 - factorized_top_k/top_100_categorical_accuracy: 0.1774 - loss: 31854.3366 - regularization_loss: 0.0000e+00 - total_loss: 31854.3366\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'factorized_top_k/top_1_categorical_accuracy': 0.0002500000118743628,\n",
       " 'factorized_top_k/top_5_categorical_accuracy': 0.002199999988079071,\n",
       " 'factorized_top_k/top_10_categorical_accuracy': 0.007000000216066837,\n",
       " 'factorized_top_k/top_50_categorical_accuracy': 0.07885000109672546,\n",
       " 'factorized_top_k/top_100_categorical_accuracy': 0.17739999294281006,\n",
       " 'loss': 28888.240234375,\n",
       " 'regularization_loss': 0,\n",
       " 'total_loss': 28888.240234375}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate model\n",
    "main_model.evaluate(cached_test, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test set performance is much worse and starts de-grading just after the first epoch. \n",
    "\n",
    "Our model is likely to perform better on the data that it has seen, simply because it can memorize it. This overfitting phenomenon is especially strong when models have many parameters. It can be mediated by model regularization and use of user and movie features that help the model generalize better to unseen data.\n",
    "\n",
    "The model is re-recommending some of users' already watched movies. These known-positive watches can crowd out test movies out of top K recommendations.\n",
    "\n",
    "The second phenomenon can be tackled by excluding previously seen movies from test recommendations. This approach is relatively common in the recommender systems literature, but we don't follow it in these tutorials. If not recommending past watches is important, we should expect appropriately specified models to learn this behaviour automatically from past user history and contextual information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow_recommenders.layers.factorized_top_k.BruteForce at 0x7fcc4047b6a0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate index\n",
    "index = tfrs.layers.factorized_top_k.BruteForce(main_model.user_model)\n",
    "index.index_from_dataset(movies.batch(100).map(lambda title: (title, main_model.movie_model(title))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for user 42: [b'Before and After (1996)' b'Jack (1996)'\n",
      " b'All Dogs Go to Heaven 2 (1996)']\n"
     ]
    }
   ],
   "source": [
    "# Get recommendations.\n",
    "_, titles = index(tf.constant([\"42\"]))\n",
    "print(f\"Recommendations for user 42: {titles[0, :3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 recommendations: tf.Tensor(\n",
      "[b'For Richer or Poorer (1997)' b'That Old Feeling (1997)'\n",
      " b'Flubber (1997)' b'Eye for an Eye (1996)' b\"Preacher's Wife, The (1996)\"], shape=(5,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# Get some recommendations\n",
    "i, titles = index(np.array([\"939\"]))\n",
    "print(\"Top 5 recommendations:\", titles[0, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this model, we created a user-movie model. However, for some applications (for example, product detail pages) it's common to perform item-to-item (for example, movie-to-movie or product-to-product) recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

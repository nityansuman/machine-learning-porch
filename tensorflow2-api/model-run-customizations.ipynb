{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Customizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import packages\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.__version__ # 2.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Loss Function\n",
    "\n",
    "There are two ways to write custom loss function:\n",
    "\n",
    "- The first example creates a function that accepts inputs y_true and y_pred.\n",
    "- Use subclassing to create custom loss function with advanced features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1: Simple and Elegent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T13:31:36.332742Z",
     "start_time": "2021-01-26T13:31:36.331084Z"
    }
   },
   "outputs": [],
   "source": [
    "def custom_mean_squared_error(y_true, y_pred):\n",
    "    return tf.math.reduce_mean(tf.square(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2: Advance\n",
    "\n",
    "If you need a loss function that takes in parameters beside y_true and y_pred, you can subclass the tf.keras.losses.Loss class and implement the following two methods:\n",
    "\n",
    "- __init__ (self): accept parameters to pass during the call of your loss function\n",
    "- call(self, y_true, y_pred): use the targets (y_true) and the model predictions (y_pred) to compute the model's loss\n",
    "\n",
    "Let's say you want to use mean squared error, but with an added term that will de-incentivize prediction values far from 0.5 (we assume that the categorical targets are one-hot encoded and take values between 0 and 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T13:31:39.293397Z",
     "start_time": "2021-01-26T13:31:39.290171Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomMSE(tf.keras.losses.Loss):\n",
    "    def __init__(self, regularization_factor=0.1, name=\"custom_mse\"):\n",
    "        super().__init__(name=name)\n",
    "        self.regularization_factor = regularization_factor\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        mse = tf.math.reduce_mean(tf.square(y_true - y_pred))\n",
    "        reg = tf.math.reduce_mean(tf.square(0.5 - y_pred))\n",
    "        return mse + reg * self.regularization_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Custom Metrics\n",
    "\n",
    "If you need a metric that isn't part of the API, you can easily create custom metrics by subclassing the tf.keras.metrics.Metric class.\n",
    "\n",
    "You will need to implement 4 methods:\n",
    "\n",
    "- __init__(self), in which you will create state variables for your metric.\n",
    "- update_state(self, y_true, y_pred, sample_weight=None), which uses the targets (y_true) and the model predictions (y_pred) to update the state variables.\n",
    "- result(self), which uses the state variables to compute the final results.\n",
    "- reset_states(self), which reinitializes the state of the metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T13:31:42.581450Z",
     "start_time": "2021-01-26T13:31:42.576818Z"
    }
   },
   "outputs": [],
   "source": [
    "class CategoricalTruePositives(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name=\"categorical_true_positives\", **kwargs):\n",
    "        super(CategoricalTruePositives, self).__init__(name=name, **kwargs)\n",
    "        self.true_positives = self.add_weight(name=\"ctp\", initializer=\"zeros\")\n",
    "    \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.reshape(tf.argmax(y_pred, axis=1), shape=(-1, 1))\n",
    "        values = tf.cast(y_true, \"int32\") == tf.cast(y_pred, \"int32\")\n",
    "        values = tf.cast(values, \"float32\")\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = tf.cast(sample_weight, \"float32\")\n",
    "            values = tf.multiply(values, sample_weight)\n",
    "        self.true_positives.assign_add(tf.reduce_sum(values))\n",
    "\n",
    "    def result(self):\n",
    "        # Actual metric value at the end of each epoch\n",
    "        return self.true_positives\n",
    "\n",
    "    def reset_states(self):\n",
    "        # The state of the metric will be reset at the start of each epoch.\n",
    "        self.true_positives.assign(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighing Samples and Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Weights\n",
    "\n",
    "This is set by passing a dictionary to the class_weight argument to Model.fit().\n",
    "\n",
    "This dictionary maps class indices to the weight that should be used for samples belonging to this class.\n",
    "This can be used to balance classes without resampling, or to train a model that gives more importance to a particular class.\n",
    "\n",
    "For instance, if class \"0\" is half as represented as class \"1\" in your data, you could use Model.fit(..., class_weight={0: 1.0, 1: 0.5})."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Weights\n",
    "\n",
    "For fine grained control, or if you are not building a classifier, you can use \"sample weights\".\n",
    "\n",
    "- When training from NumPy data: Pass the sample_weight argument to Model.fit().\n",
    "- When training from tf.data or any other sort of iterator: Yield (input_batch, label_batch, sample_weight_batch) tuples.\n",
    "\n",
    "A \"sample weights\" array is an array of numbers that specify how much weight each sample in a batch should have in computing the total loss.\n",
    "It is commonly used in imbalanced classification problems (the idea being to give more weight to rarely-seen classes).\n",
    "\n",
    "When the weights used are ones and zeros, the array can be used as a mask for the loss function (entirely discarding the contribution of certain samples to the total loss)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks\n",
    "\n",
    "Callbacks are objects that are called at different points during training (at the start of an epoch, at the end of a batch, at the end of an epoch, etc.):\n",
    "\n",
    "- Doing validation at different points during training (beyond the built-in per-epoch validation)\n",
    "- Checkpointing the model at regular intervals or when it exceeds a certain accuracy threshold\n",
    "- Changing the learning rate of the model when training seems to be plateauing\n",
    "- Stopping training when validation loss starts increasing\n",
    "- Doing fine-tuning of the top layers when training seems to be plateauing\n",
    "- Sending email or instant message notifications when training ends or where a certain performance threshold is exceeded etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T13:32:14.857442Z",
     "start_time": "2021-01-26T13:32:06.574960Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a callback to monitor validation loss for early stopping\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        # Stop training when `val_loss` is no longer improving\n",
    "        monitor=\"val_loss\",\n",
    "        # \"no longer improving\" being defined as \"no better than 1e-2 less\"\n",
    "        min_delta=1e-2,\n",
    "        # \"no longer improving\" being further defined as \"for at least 2 epochs\"\n",
    "        patience=2,\n",
    "        verbose=1,\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Callback\n",
    "\n",
    "You can create a custom callback by extending the base class tf.keras.callbacks.Callback. A callback has access to its associated model through the class property self.model.\n",
    "\n",
    "Here's a sample example saving a list of per-batch loss values during training instead of default behaviour of saving it for every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T13:32:14.873179Z",
     "start_time": "2021-01-26T13:32:14.863784Z"
    }
   },
   "outputs": [],
   "source": [
    "class LossHistoryBatch(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs):\n",
    "        self.per_batch_losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.per_batch_losses.append(logs.get(\"loss\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Checkpointing via Callbacks\n",
    "\n",
    "When you're training model on relatively large datasets, it's crucial to save checkpoints of your model at frequent intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T13:32:33.013595Z",
     "start_time": "2021-01-26T13:32:23.143839Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a callback to save models while monitoring validation loss\n",
    "# It overwrites the model when validation loss improves\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"tmp/mymodel_{epoch}\", # Path to save model\n",
    "        save_best_only=True,  # Overwrite a model only if `val_loss` has improved.\n",
    "        monitor=\"val_loss\",\n",
    "        verbose=1,\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Scheduler via Callbacks\n",
    "\n",
    "A common pattern when training deep learning models is to gradually reduce the learning as training progresses. This is generally known as \"learning rate decay\".\n",
    "\n",
    "The learning decay schedule could be static (fixed in advance, as a function of the current epoch or the current batch index), or dynamic (responding to the current behavior of the model, in particular the validation loss)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static\n",
    "\n",
    "You can easily use a static learning rate decay schedule by passing a schedule object as the learning_rate argument in your optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T13:32:33.017518Z",
     "start_time": "2021-01-26T13:32:33.014843Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set initial learning rate\n",
    "initial_learning_rate = 0.01\n",
    "\n",
    "# Define a scheduler\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=100000,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "# Define optimizer with a learning rate scheduler\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=lr_schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic\n",
    "\n",
    "A dynamic learning rate schedule (for instance, decreasing the learning rate when the validation loss is no longer improving) cannot be achieved with these schedule objects since the optimizer does not have access to validation metrics.\n",
    "\n",
    "However, callbacks do have access to all metrics, including validation metrics! You can thus achieve this pattern by using a callback that modifies the current learning rate on the optimizer. In fact, this is even built-in as the ReduceLROnPlateau callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T13:32:33.029225Z",
     "start_time": "2021-01-26T13:32:33.018752Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as k\n",
    "\n",
    "# A very simple dynamic learning rate scheduler using callbacks\n",
    "# Note that this is just for the sake of example\n",
    "class IncreaseLR(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        lr = k.get_value(self.model.optimizer.lr)\n",
    "        new_lr = lr + 0.001 # Decrease learning rate\n",
    "        k.set_value(self.model.optimizer.lr, new_lr) # Set new learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python [conda env:dl-env]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
